{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. VOLO\n",
    "\n",
    "This notebook is written to better understand volo architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Method\n",
    "\n",
    "Our model can be regarded as an architecture with two seperate stages. The first stage consists of a stack of Outlookers that generates fine-level token representations. The second stage deploys a sequence of transformer blocks to aggregate global information. At the beginning of each stage, a patch embedding module is used to map the input to token representations with designed shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Outlooker\n",
    "\n",
    "Given a sequence of input C-dim token representations `X.shape = (H, W, C)`, Outlooker can be written as follows: \n",
    "- `X^hat = OutlookAtt(LN(X)) + X`\n",
    "- `Z=MLP(LN(X^hat)) + X^hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Outlook Attention\n",
    "\n",
    "Outlook attention is simple, efficient, and easy to implement. The main insights behind it are:\n",
    "1. the feature at each spatial location is representative enough to generate attention weights for locally aggregating its neighboring features.\n",
    "2. The dense and local spatial aggregation can encode fine-level information efficiently.\n",
    "\n",
    "![volo_211](./imgs/volo_211.jpg)\n",
    "![volo_211_2](./imgs/volo_211_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 0.34M\n",
      "torch.Size([2, 196, 192])\n",
      "# params: 1.67M\n",
      "torch.Size([2, 196, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class OutLookAttn(nn.Module):\n",
    "    def __init__(self, dim, head, H, W, K=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.v_pj = nn.Linear(dim, dim)\n",
    "        self.attn = nn.Linear(dim, head * K ** 4)\n",
    "        self.unfold = nn.Unfold(K, padding=padding)\n",
    "        self.fold = nn.Fold((H, W), K, padding=padding)\n",
    "        self.K = K\n",
    "        self.head = head\n",
    "        self.dim = dim\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, x): # input(x): (B, H * W, dim)\n",
    "        # value(v): (B, H * W, dim) -> (B, dim, H, W) -> (B, H * W, head, K**2, dim / head)\n",
    "        v = self.v_pj(x).permute(0, 2, 1).reshape(-1, self.dim, self.H, self.W)\n",
    "        v = self.unfold(v).reshape(-1, self.head, self.dim // self.head, self.K ** 2, self.H * self.W)\n",
    "        v = v.permute(0, 4, 1, 3, 2) \n",
    "\n",
    "        # attention(a): (B, H * W, dim) -> (B, H * W, head, K ** 2, K ** 2)\n",
    "        a = self.attn(x).reshape(-1, self.H * self.W, self.head, self.K ** 2, self.K ** 2)\n",
    "        a = F.softmax(a, dim=-1) \n",
    "\n",
    "        x = (a @ v).permute(0, 2, 4, 3, 1).reshape(-1, self.dim * (self.K ** 2), self.H * self.W)\n",
    "        x = self.fold(x).permute(0, 2, 3, 1).reshape(-1, self.H * self.W, self.dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "        self.fc2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Outlooker(nn.Module):\n",
    "    def __init__(self, dim, head, mlp_ratio, H, W, K=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.outlook_attn = OutLookAttn(dim, head, H, W, K, padding)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.ln1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.outlook_attn(self.ln1(x)) + x\n",
    "        z = self.mlp(self.ln2(x_hat)) + x_hat\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, H, W, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, patch_size, patch_size)\n",
    "        self.patch_len = H * W // (patch_size * patch_size)\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim != 4:\n",
    "            x = x.permute(0, 2, 1).reshape(-1, self.in_dim, self.H, self.W)\n",
    "        x = self.conv(x).permute(0, 2, 3, 1).reshape(-1, self.patch_len, self.out_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio, head):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassAttention(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio, head):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class VOLO(nn.Module):\n",
    "    def __init__(self, s1_num, s1_dim, s1_head, s1_mlp_ratio, s2_num, s2_dim, s2_head, s2_mlp_ratio,\n",
    "                H, W, K=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding1 = PatchEmbedding(3, s1_dim, H, W, 8)\n",
    "        self.stage1 = nn.Sequential(*[Outlooker(s1_dim, s1_head, s1_mlp_ratio, H // 8, W // 8, K, padding) for _ in range(s1_num)])\n",
    "        self.patch_embedding2 = PatchEmbedding(s1_dim, s2_dim, H // 8, W // 8, 2)\n",
    "        self.stage2 = nn.Sequential(*[SelfAttention(s2_dim, s2_mlp_ratio, s2_head) for _ in range(s2_num)])\n",
    "        self.cls = ClassAttention(s2_dim, s2_mlp_ratio, s2_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(self.patch_embedding1(x))\n",
    "        x = self.stage2(self.patch_embedding2(x))\n",
    "        x = self.cls(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "outlook_attn = Outlooker(192, 6, 3, 14, 14)\n",
    "x = torch.rand([2, 14 * 14, 192])\n",
    "y = outlook_attn(x)\n",
    "print(f\"# params: {sum([p.numel() for p in outlook_attn.parameters() if p.requires_grad])/(1024*1024):.2f}M\")\n",
    "print(y.shape)\n",
    "\n",
    "model = VOLO(H=224, W=224, s1_num=4, s1_dim=192, s1_head=6, s1_mlp_ratio=3, \n",
    "                            s2_num=14, s2_dim=384, s2_head=12, s2_mlp_ratio=3)\n",
    "x = torch.rand([2, 3, 224, 224])\n",
    "y = model(x)\n",
    "print(f\"# params: {sum([p.numel() for p in model.parameters() if p.requires_grad])/(1024*1024):.2f}M\")\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
