{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. VOLO\n",
    "\n",
    "This notebook is written to better understand volo architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Method\n",
    "\n",
    "Our model can be regarded as an architecture with two seperate stages. The first stage consists of a stack of Outlookers that generates fine-level token representations. The second stage deploys a sequence of transformer blocks to aggregate global information. At the beginning of each stage, a patch embedding module is used to map the input to token representations with designed shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Outlooker\n",
    "\n",
    "Given a sequence of input C-dim token representations `X.shape = (H, W, C)`, Outlooker can be written as follows: \n",
    "- `X^hat = OutlookAtt(LN(X)) + X`\n",
    "- `Z=MLP(LN(X^hat)) + X^hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Outlook Attention\n",
    "\n",
    "Outlook attention is simple, efficient, and easy to implement. The main insights behind it are:\n",
    "1. the feature at each spatial location is representative enough to generate attention weights for locally aggregating its neighboring features.\n",
    "2. The dense and local spatial aggregation can encode fine-level information efficiently.\n",
    "\n",
    "![volo_211](./imgs/volo_211.jpg)\n",
    "![volo_211_2](./imgs/volo_211_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "1. OutLookAttn 기존 모델 코드와 비교\n",
    "2. 전체적으로 들어가는 Dropout / Attention Dropout 추가\n",
    "3. 코드 로직 / 빼먹은 부분은 없는지 확인\n",
    "4. 깔끔하게 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 0.37M\n",
      "torch.Size([2, 196, 192])\n",
      "volo_d1_224 # params: 25.40M\n",
      "torch.Size([2, 1, 1000])\n",
      "volo_d2_224 # params: 55.96M\n",
      "torch.Size([2, 1, 1000])\n",
      "volo_d3_224 # params: 82.33M\n",
      "torch.Size([2, 1, 1000])\n",
      "volo_d4_224 # params: 184.02M\n",
      "torch.Size([2, 1, 1000])\n",
      "volo_d5_224 # params: 281.77M\n",
      "torch.Size([2, 1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class OutLookAttn(nn.Module):\n",
    "    \"\"\"OutLookAttention (need to compare with original code)\"\"\"\n",
    "    def __init__(self, dim, head, H, W, K=3, padding=1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.v_pj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn = nn.Linear(dim, head * K ** 4)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.unfold = nn.Unfold(K, padding=padding)\n",
    "        self.fold = nn.Fold((H, W), K, padding=padding)\n",
    "        self.K = K\n",
    "        self.head = head\n",
    "        self.dim = dim\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, x): # input(x): (B, H * W, dim)\n",
    "        # value(v): (B, H * W, dim) -> (B, dim, H, W) -> (B, H * W, head, K**2, dim / head)\n",
    "        v = self.v_pj(x).permute(0, 2, 1).reshape(-1, self.dim, self.H, self.W)\n",
    "        v = self.unfold(v).reshape(-1, self.head, self.dim // self.head, self.K ** 2, self.H * self.W)\n",
    "        v = v.permute(0, 4, 1, 3, 2) \n",
    "\n",
    "        # attention(a): (B, H * W, dim) -> (B, H * W, head, K ** 2, K ** 2)\n",
    "        a = self.attn(x).reshape(-1, self.H * self.W, self.head, self.K ** 2, self.K ** 2)\n",
    "        a = F.softmax(a, dim=-1) \n",
    "\n",
    "        x = (a @ v).permute(0, 2, 4, 3, 1).reshape(-1, self.dim * (self.K ** 2), self.H * self.W)\n",
    "        x = self.fold(x).permute(0, 2, 3, 1).reshape(-1, self.H * self.W, self.dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "        self.fc2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Outlooker(nn.Module):\n",
    "    def __init__(self, dim, head, mlp_ratio, H, W, K=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.outlook_attn = OutLookAttn(dim, head, H, W, K, padding)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.ln1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.outlook_attn(self.ln1(x)) + x\n",
    "        z = self.mlp(self.ln2(x_hat)) + x_hat\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class ConvNormAct(nn.Sequential):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding, stride):\n",
    "        super().__init__(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, bias=False),\n",
    "                        nn.BatchNorm2d(out_dim), nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Non-overlapping embedding function\n",
    "\n",
    "    difference from paper (from author repo)\n",
    "    - use 4 conv layer in first patch embedding (img -> (pe) -> stage1)\n",
    "    - add positional embedding in second patch embedding (stage1 -> (pe) -> stage2)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, H, W, patch_size, use_stem=False, hidden_dim=64, add_pe=False):\n",
    "        super().__init__()\n",
    "        if use_stem:\n",
    "            self.conv = nn.Sequential(\n",
    "                ConvNormAct(in_dim, hidden_dim, kernel_size=7, padding=3, stride=2),\n",
    "                ConvNormAct(hidden_dim, hidden_dim, kernel_size=3, padding=1, stride=1),\n",
    "                ConvNormAct(hidden_dim, hidden_dim, kernel_size=3, padding=1, stride=1),\n",
    "                nn.Conv2d(hidden_dim, out_dim, patch_size // 2, patch_size // 2)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_dim, out_dim, patch_size, patch_size)\n",
    "\n",
    "        self.patch_len = H * W // (patch_size * patch_size)\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.add_pe = add_pe\n",
    "        if self.add_pe:\n",
    "            self.pe = nn.Parameter(torch.zeros([1, self.patch_len, self.out_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim != 4:\n",
    "            x = x.permute(0, 2, 1).reshape(-1, self.in_dim, self.H, self.W)\n",
    "        x = self.conv(x).permute(0, 2, 3, 1).reshape(-1, self.patch_len, self.out_dim)\n",
    "\n",
    "        if self.add_pe:\n",
    "            x = x + self.pe.expand(x.size(0), -1, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.k = dim // head\n",
    "        self.div = math.sqrt(self.k)\n",
    "        self.head = head\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = [out.reshape(B, N, self.head, self.k).permute(0, 2, 1, 3) for out in self.qkv(x).tensor_split(3, dim=-1)]\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) / self.div\n",
    "        attn_prob = F.softmax(attn, dim=-1)\n",
    "\n",
    "        out = attn_prob @ v\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio, head):\n",
    "        super().__init__()\n",
    "        self.attn = MHSA(dim, head)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x\n",
    "        x = self.mlp(self.norm2(x)) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHCA(nn.Module):\n",
    "    def __init__(self, dim, head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.k = dim // head\n",
    "        self.div = math.sqrt(self.k)\n",
    "        self.head = head\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q = self.q(x[:, :1]).reshape(B, 1, self.head, self.k).permute(0, 2, 1, 3)\n",
    "        k, v = [out.reshape(B, N-1, self.head, self.k).permute(0, 2, 1, 3) for out in self.kv(x[:, 1:]).tensor_split(2, dim=-1)]\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) / self.div\n",
    "        attn_prob = F.softmax(attn, dim=-1)\n",
    "\n",
    "        out = attn_prob @ v\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, 1, D)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ClassAttention(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio, head):\n",
    "        super().__init__()\n",
    "        self.attn = MHCA(dim, head)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, cls_x):\n",
    "        cls, x = cls_x\n",
    "        z = torch.concat([cls, x], dim=1)\n",
    "        cls = self.attn(self.norm1(z)) + cls # Todo: check whether norm1() is applied to z or cls\n",
    "        cls = self.mlp(self.norm2(cls)) + cls\n",
    "\n",
    "        return cls, x\n",
    "\n",
    "\n",
    "class VOLO(nn.Module):\n",
    "    def __init__(self, num_classes, s1_num, s1_dim, s1_head, s1_mlp_ratio, s2_num, s2_dim, s2_head, s2_mlp_ratio,\n",
    "                H, W, K=3, padding=1, stem_hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Forward: pe1 (0.04M) -> stage1 (1.35M) -> pe2 (0.28M) -> stage2 (19.75M) -> cls (2.82M) -> norm -> fc (0.37M)\n",
    "        self.patch_embedding1 = PatchEmbedding(3, s1_dim, H, W, 8, use_stem=True, hidden_dim=stem_hidden_dim)\n",
    "        self.patch_embedding2 = PatchEmbedding(s1_dim, s2_dim, H // 8, W // 8, 2, add_pe=True)\n",
    "        self.stage1 = nn.Sequential(*[Outlooker(s1_dim, s1_head, s1_mlp_ratio, H // 8, W // 8, K, padding) for _ in range(s1_num)])\n",
    "        self.stage2 = nn.Sequential(*[SelfAttention(s2_dim, s2_mlp_ratio, s2_head) for _ in range(s2_num)])\n",
    "        self.cls = nn.Sequential(*[ClassAttention(s2_dim, s2_mlp_ratio, s2_head) for _ in range(2)])\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, s2_dim))\n",
    "        self.norm = nn.LayerNorm(s2_dim)\n",
    "        self.classifier = nn.Linear(s2_dim, num_classes)\n",
    "        self.aux_head = nn.Linear(s2_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(self.patch_embedding1(x))\n",
    "        x = self.stage2(self.patch_embedding2(x))\n",
    "        cls_token, x = self.cls((self.cls_token.expand(x.size(0), -1, -1), x))\n",
    "        cls_token = self.norm(cls_token)\n",
    "        out = self.classifier(cls_token)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def volo_d1_224():\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=4, s1_dim=192, s1_head=6, s1_mlp_ratio=3, \n",
    "            s2_num=14, s2_dim=384, s2_head=12, s2_mlp_ratio=3)\n",
    "\n",
    "def volo_d2_224(): # change: increase layer & dim & head\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=6, s1_dim=256, s1_head=8, s1_mlp_ratio=3, \n",
    "            s2_num=18, s2_dim=512, s2_head=16, s2_mlp_ratio=3)\n",
    "\n",
    "def volo_d3_224(): # change: increase layer\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=8, s1_dim=256, s1_head=8, s1_mlp_ratio=3, \n",
    "            s2_num=28, s2_dim=512, s2_head=16, s2_mlp_ratio=3)\n",
    "\n",
    "def volo_d4_224(): # change: increase dim & head\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=8, s1_dim=384, s1_head=12, s1_mlp_ratio=3, \n",
    "            s2_num=28, s2_dim=768, s2_head=16, s2_mlp_ratio=3)\n",
    "\n",
    "def volo_d5_224(): # change: increase layer & mlp ratio \n",
    "    \"\"\"volo d5 @ 224\n",
    "    \n",
    "    modification from paper (from author code)\n",
    "    - stem_hidden_dim = 128\n",
    "    \"\"\"\n",
    "    return VOLO(num_classes=1000, H=224, W=224, stem_hidden_dim=128,\n",
    "            s1_num=12, s1_dim=384, s1_head=12, s1_mlp_ratio=4, \n",
    "            s2_num=36, s2_dim=768, s2_head=16, s2_mlp_ratio=4)\n",
    "\n",
    "\n",
    "outlook_attn = Outlooker(192, 6, 3, 14, 14)\n",
    "x = torch.rand([2, 14 * 14, 192])\n",
    "y = outlook_attn(x)\n",
    "print(f\"# params: {sum([p.numel() for p in outlook_attn.parameters() if p.requires_grad])/(1024*1024):.2f}M\")\n",
    "print(y.shape)\n",
    "\n",
    "for model_fn in [volo_d1_224, volo_d2_224, volo_d3_224, volo_d4_224, volo_d5_224]:\n",
    "    model = model_fn()\n",
    "    x = torch.rand([2, 3, 224, 224])\n",
    "    y = model(x)\n",
    "    print(f\"{model_fn.__name__} # params: {sum([p.numel() for p in model.parameters() if p.requires_grad])/(1024*1024):.2f}M\")\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
