{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. VOLO\n",
    "\n",
    "This notebook is written to better understand volo architecture.\n",
    "\n",
    "**Index**\n",
    "1. Model definition\n",
    "2. Check parameter & Flops\n",
    "3. Train with CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "from pic.model import register_model\n",
    "\n",
    "\n",
    "class OutLookAttn(nn.Module):\n",
    "    \"\"\"Outlook Attention\n",
    "    \n",
    "    modification from paper (copied from author code): \n",
    "    - apply stride(2) option to reduce flops (maybe increase performance also)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head, H, W, K=3, padding=1, stride=2, qkv_bias=False, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.v_pj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn = nn.Conv2d(dim, head * K ** 4, 1)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.unfold = nn.Unfold(K, padding=padding, stride=stride)\n",
    "        self.fold = nn.Fold((H, W), K, padding=padding, stride=stride)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.K = K\n",
    "        self.H = H \n",
    "        self.W = W \n",
    "        self.head = head\n",
    "        self.dim = dim\n",
    "\n",
    "        self.scale = (dim / head) ** -0.5\n",
    "        self.seq_len = (H // stride) * (H // stride)\n",
    "\n",
    "    def forward(self, x): # input(x): (B, H * W, dim)\n",
    "        # value(v): (B, H * W, dim) -> (B, dim, H, W) -> (B, H * W // stride ** 2, head, K**2, dim / head)\n",
    "        v = self.v_pj(x).permute(0, 2, 1).reshape(-1, self.dim, self.H, self.W)\n",
    "        v = self.unfold(v).reshape(-1, self.head, self.dim // self.head, self.K ** 2, self.seq_len)\n",
    "        v = v.permute(0, 4, 1, 3, 2) \n",
    "\n",
    "        # attention(a): (B, H * W, dim) -> (B, H * W // stride ** 2, head, K ** 2, K ** 2)\n",
    "        a = self.attn(self.avg_pool(x.permute(0, 2, 1).reshape(-1, self.dim, self.H, self.W)))\n",
    "        a = a.permute(0, 2, 3, 1).reshape(-1, self.seq_len, self.head, self.K ** 2, self.K ** 2)\n",
    "        a = F.softmax(a * self.scale, dim=-1) \n",
    "        a = self.attn_drop(a)\n",
    "\n",
    "        x = (a @ v).permute(0, 2, 4, 3, 1).reshape(-1, self.dim * (self.K ** 2), self.seq_len)\n",
    "        x = self.fold(x).permute(0, 2, 3, 1).reshape(-1, self.H * self.W, self.dim)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "        self.fc2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Outlooker(nn.Module):\n",
    "    \"\"\"Outlook Attention + MLP\n",
    "    \n",
    "    details:\n",
    "    1. qkv_bias = False\n",
    "    2. attention dropout = 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, head, mlp_ratio, H, W, K=3, padding=1, stride=2, qkv_bias=False, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.outlook_attn = OutLookAttn(dim, head, H, W, K, padding, stride, qkv_bias, attn_drop)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.ln1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.outlook_attn(self.ln1(x)) + x\n",
    "        z = self.mlp(self.ln2(x_hat)) + x_hat\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class ConvNormAct(nn.Sequential):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding, stride):\n",
    "        super().__init__(nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, bias=False),\n",
    "                        nn.BatchNorm2d(out_dim), nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Non-overlapping embedding function\n",
    "\n",
    "    difference from paper (copied from author repo)\n",
    "    - use 4 conv layer in first patch embedding (img -> (pe) -> stage1)\n",
    "    - add positional embedding in second patch embedding (stage1 -> (pe) -> stage2)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, H, W, patch_size, use_stem=False, hidden_dim=64, add_pe=False):\n",
    "        super().__init__()\n",
    "        if use_stem:\n",
    "            self.conv = nn.Sequential(\n",
    "                ConvNormAct(in_dim, hidden_dim, kernel_size=7, padding=3, stride=2),\n",
    "                ConvNormAct(hidden_dim, hidden_dim, kernel_size=3, padding=1, stride=1),\n",
    "                ConvNormAct(hidden_dim, hidden_dim, kernel_size=3, padding=1, stride=1),\n",
    "                nn.Conv2d(hidden_dim, out_dim, patch_size // 2, patch_size // 2)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(in_dim, out_dim, patch_size, patch_size)\n",
    "\n",
    "        self.patch_len = H * W // (patch_size * patch_size)\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.add_pe = add_pe\n",
    "        if self.add_pe:\n",
    "            self.pe = nn.Parameter(torch.zeros([1, self.patch_len, self.out_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim != 4:\n",
    "            x = x.permute(0, 2, 1).reshape(-1, self.in_dim, self.H, self.W)\n",
    "        x = self.conv(x).permute(0, 2, 3, 1).reshape(-1, self.patch_len, self.out_dim)\n",
    "\n",
    "        if self.add_pe:\n",
    "            x = x + self.pe.expand(x.size(0), -1, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, head, qkv_bias=False, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.k = dim // head\n",
    "        self.div = math.sqrt(self.k)\n",
    "        self.head = head\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = [out.reshape(B, N, self.head, self.k).permute(0, 2, 1, 3) for out in self.qkv(x).tensor_split(3, dim=-1)]\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) / self.div\n",
    "        attn_prob = F.softmax(attn, dim=-1)\n",
    "        attn_prob = self.attn_drop(attn_prob)\n",
    "\n",
    "        out = attn_prob @ v\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self Attention\n",
    "    \n",
    "    Details: drop_path_rate is only applied to this module\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, mlp_ratio, head, qkv_bias=False, attn_drop=0.0, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = MHSA(dim, head, qkv_bias, attn_drop)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop_path(self.attn(self.norm1(x))) + x\n",
    "        x = self.drop_path(self.mlp(self.norm2(x))) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHCA(nn.Module):\n",
    "    def __init__(self, dim, head, qkv_bias=False, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.k = dim // head\n",
    "        self.div = math.sqrt(self.k)\n",
    "        self.head = head\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q = self.q(x[:, :1]).reshape(B, 1, self.head, self.k).permute(0, 2, 1, 3)\n",
    "        k, v = [out.reshape(B, N-1, self.head, self.k).permute(0, 2, 1, 3) for out in self.kv(x[:, 1:]).tensor_split(2, dim=-1)]\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) / self.div\n",
    "        attn_prob = F.softmax(attn, dim=-1)\n",
    "        attn_prob = self.attn_drop(attn_prob)\n",
    "\n",
    "        out = attn_prob @ v\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, 1, D)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ClassAttention(nn.Module):\n",
    "    \"\"\"Class Attention\"\"\"\n",
    "    def __init__(self, dim, mlp_ratio, head, qkv_bias=False, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = MHCA(dim, head, qkv_bias, attn_drop)\n",
    "        self.mlp = MLP(dim, mlp_ratio)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, cls_x):\n",
    "        cls, x = cls_x\n",
    "        z = torch.concat([cls, x], dim=1)\n",
    "        cls = self.attn(self.norm1(z)) + cls\n",
    "        cls = self.mlp(self.norm2(cls)) + cls\n",
    "\n",
    "        return cls, x\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    'volo_d1_224': {'parameter': dict(H=224, W=224, \n",
    "    s1_num=4, s1_dim=192, s1_head=6, s1_mlp_ratio=3,\n",
    "    s2_num=14, s2_dim=384, s2_head=12, s2_mlp_ratio=3), 'etc':{}},\n",
    "    'volo_d2_224': {'parameter': dict(H=224, W=224, \n",
    "    s1_num=6, s1_dim=256, s1_head=8, s1_mlp_ratio=3,\n",
    "    s2_num=18, s2_dim=512, s2_head=16, s2_mlp_ratio=3), 'etc':{}},\n",
    "    'volo_d3_224': {'parameter': dict(H=224, W=224, \n",
    "    s1_num=8, s1_dim=256, s1_head=8, s1_mlp_ratio=3,\n",
    "    s2_num=28, s2_dim=512, s2_head=16, s2_mlp_ratio=3), 'etc':{}},\n",
    "    'volo_d4_224': {'parameter': dict(H=224, W=224, \n",
    "    s1_num=8, s1_dim=384, s1_head=12, s1_mlp_ratio=3,\n",
    "    s2_num=28, s2_dim=768, s2_head=16, s2_mlp_ratio=3), 'etc':{}},\n",
    "    'volo_d5_224': {'parameter': dict(H=224, W=224, stem_hidden_dim=128, \n",
    "    s1_num=12, s1_dim=384, s1_head=12, s1_mlp_ratio=4,\n",
    "    s2_num=36, s2_dim=768, s2_head=16, s2_mlp_ratio=4), 'etc':{}},\n",
    "}\n",
    "\n",
    "\n",
    "@register_model\n",
    "class VOLO(nn.Module):\n",
    "    def __init__(self, num_classes, s1_num, s1_dim, s1_head, s1_mlp_ratio, s2_num, s2_dim, s2_head, s2_mlp_ratio,\n",
    "                H, W, K=3, padding=1, stride=2, stem_hidden_dim=64, use_token_label=True, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.patch_embedding1 = PatchEmbedding(3, s1_dim, H, W, 8, use_stem=True, hidden_dim=stem_hidden_dim)\n",
    "        self.patch_embedding2 = PatchEmbedding(s1_dim, s2_dim, H // 8, W // 8, 2, add_pe=True)\n",
    "        self.stage1 = nn.Sequential(*[Outlooker(s1_dim, s1_head, s1_mlp_ratio, H // 8, W // 8, K, padding, stride) for _ in range(s1_num)])\n",
    "        self.stage2 = nn.Sequential(*[SelfAttention(s2_dim, s2_mlp_ratio, s2_head, drop_path_rate=drop_path_rate * (i / s2_num)) for i in range(s2_num)])\n",
    "        self.cls = nn.Sequential(*[ClassAttention(s2_dim, s2_mlp_ratio, s2_head) for _ in range(2)])\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, s2_dim))\n",
    "        self.norm = nn.LayerNorm(s2_dim)\n",
    "        self.classifier = nn.Linear(s2_dim, num_classes)\n",
    "\n",
    "        # To check if params & flops are matched with timm version\n",
    "        self.use_token_label = use_token_label\n",
    "        if self.use_token_label:\n",
    "            self.aux_head = nn.Linear(s2_dim, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(self.patch_embedding1(x))\n",
    "        x = self.stage2(self.patch_embedding2(x))\n",
    "        cls_token, x = self.cls((self.cls_token.expand(x.size(0), -1, -1), x))\n",
    "        cls_token = self.norm(cls_token)\n",
    "        out = self.classifier(cls_token)\n",
    "\n",
    "        # To check if params & flops are matched with timm version\n",
    "        if self.use_token_label:\n",
    "            x_aux = self.aux_head(x[:, 1:])\n",
    "            out = out + 0.5 * x_aux.max(1)[0]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def volo_d1_224(**kwargs):\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=4, s1_dim=192, s1_head=6, s1_mlp_ratio=3, \n",
    "            s2_num=14, s2_dim=384, s2_head=12, s2_mlp_ratio=3, **kwargs)\n",
    "\n",
    "def volo_d2_224(**kwargs): # change from d1: increase layer & dim & head\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=6, s1_dim=256, s1_head=8, s1_mlp_ratio=3, \n",
    "            s2_num=18, s2_dim=512, s2_head=16, s2_mlp_ratio=3, **kwargs)\n",
    "\n",
    "def volo_d3_224(**kwargs): # change from d2: increase layer\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=8, s1_dim=256, s1_head=8, s1_mlp_ratio=3, \n",
    "            s2_num=28, s2_dim=512, s2_head=16, s2_mlp_ratio=3, **kwargs)\n",
    "\n",
    "def volo_d4_224(**kwargs): # change from d3: increase dim & head\n",
    "    return VOLO(num_classes=1000, H=224, W=224, \n",
    "            s1_num=8, s1_dim=384, s1_head=12, s1_mlp_ratio=3, \n",
    "            s2_num=28, s2_dim=768, s2_head=16, s2_mlp_ratio=3, **kwargs)\n",
    "\n",
    "def volo_d5_224(**kwargs): # change from d4: increase layer & mlp ratio \n",
    "    \"\"\"volo d5 @ 224\n",
    "    \n",
    "    modification from paper (copied from author code)\n",
    "    - stem_hidden_dim = 128\n",
    "    \"\"\"\n",
    "    return VOLO(num_classes=1000, H=224, W=224, stem_hidden_dim=128,\n",
    "            s1_num=12, s1_dim=384, s1_head=12, s1_mlp_ratio=4, \n",
    "            s2_num=36, s2_dim=768, s2_head=16, s2_mlp_ratio=4, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check parameters & gmacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                flops(G)    param(M)\n",
      "-----------------  ----------  ----------\n",
      "(ours)volo_d1_224        6.05       25.4\n",
      "(ours)volo_d2_224       12.64       55.96\n",
      "(ours)volo_d3_224       18.25       82.33\n",
      "(ours)volo_d4_224       39.64      184.02\n",
      "(ours)volo_d5_224       65.29      281.77\n",
      "(timm)volo_d1_224        6.05       25.4\n",
      "(timm)volo_d2_224       12.64       55.96\n",
      "(timm)volo_d3_224       18.25       82.33\n",
      "(timm)volo_d4_224       39.64      184.02\n",
      "(timm)volo_d5_224       65.29      281.77\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "\n",
    "from tabulate import tabulate\n",
    "from timm import create_model\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "\n",
    "def compute_flops(model):\n",
    "    return get_model_profile(\n",
    "        model=model.cuda(),\n",
    "        input_res=(1, 3, 224, 224),\n",
    "        print_profile=False,\n",
    "        detailed=False,\n",
    "        warm_up=10,\n",
    "        as_string=False,\n",
    "        output_file=None,\n",
    "        ignore_modules=None\n",
    "    )[1] / (1024*1024*1024)\n",
    "\n",
    "\n",
    "def compute_param(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])/(1024*1024)\n",
    "\n",
    "\n",
    "# for debug outlooker block\n",
    "# outlook_attn = Outlooker(192, 6, 3, 14, 14)\n",
    "# x = torch.rand([2, 14 * 14, 192])\n",
    "# y = outlook_attn(x)\n",
    "# print(f\"# params: {sum([p.numel() for p in outlook_attn.parameters() if p.requires_grad])/(1024*1024):.2f}M\")\n",
    "# print(y.shape)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_fn in [volo_d1_224, volo_d2_224, volo_d3_224, volo_d4_224, volo_d5_224]:\n",
    "    # To match with Timm version, you should set `use_token_label=True`\n",
    "    model = model_fn(use_token_label=True)\n",
    "    x = torch.rand([2, 3, 224, 224])\n",
    "    y = model(x)\n",
    "    flops = compute_flops(model)\n",
    "    params = compute_param(model)\n",
    "    results.append([f'(ours){model_fn.__name__}', round(flops, 2), round(params, 2)])\n",
    "\n",
    "for model_name in ['volo_d1_224', 'volo_d2_224', 'volo_d3_224', 'volo_d4_224', 'volo_d5_224']:\n",
    "    model = create_model(model_name)\n",
    "    x = torch.rand([2, 3, 224, 224])\n",
    "    y = model(x)\n",
    "    flops = compute_flops(model)\n",
    "    params = compute_param(model)\n",
    "    results.append([f'(timm){model_name}', round(flops, 2), round(params, 2)])\n",
    "\n",
    "print(tabulate(results, headers=['model', 'flops(G)', 'param(M)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train VOLO in CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "from pic.utils import setup, get_args_parser, save_checkpoint, resume_from_checkpoint\n",
    "from pic.utils import print_metadata, Result\n",
    "from pic.data import get_dataset, get_dataloader\n",
    "from pic.model import get_model, get_ema_ddp_model\n",
    "from pic.criterion import get_scaler_criterion\n",
    "from pic.optimizer import get_optimizer_and_scheduler\n",
    "from pic.use_case import validate, train_one_epoch\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    # 0. init ddp & logger\n",
    "    setup(args)\n",
    "\n",
    "    # 1. load dataset\n",
    "    train_dataset, valid_dataset = get_dataset(args)\n",
    "    train_dataloader, valid_dataloader = get_dataloader(train_dataset, valid_dataset, args)\n",
    "\n",
    "    # 2. make model\n",
    "    model = get_model(args)\n",
    "    model, ema_model, ddp_model = get_ema_ddp_model(model, args)\n",
    "\n",
    "    # 3. load optimizer\n",
    "    optimizer, scheduler = get_optimizer_and_scheduler(model, args)\n",
    "\n",
    "    # 4. load criterion\n",
    "    criterion, valid_criterion, scaler = get_scaler_criterion(args)\n",
    "\n",
    "    # 5. print metadata\n",
    "    print_metadata(model, train_dataset, valid_dataset, args)\n",
    "\n",
    "    # 6. control logic for checkpoint & validate\n",
    "    if args.resume:\n",
    "        start_epoch = resume_from_checkpoint(args.checkpoint_path, optimizer, scaler, scheduler)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    start_epoch = args.start_epoch if args.start_epoch else start_epoch\n",
    "    end_epoch = args.end_epoch if args.end_epoch else args.epoch\n",
    "\n",
    "    if scheduler is not None and start_epoch:\n",
    "        # Todo: sequential lr does not support step with epoch as positional variable\n",
    "        scheduler.step(start_epoch)\n",
    "\n",
    "    if args.validate_only:\n",
    "        validate(valid_dataloader, model, valid_criterion, args, 'org')\n",
    "        if args.ema:\n",
    "            validate(valid_dataloader, ema_model, valid_criterion, args, 'ema')\n",
    "        return\n",
    "\n",
    "    # 7. train\n",
    "    best_epoch = 0\n",
    "    best_acc = 0\n",
    "    top1_list = []\n",
    "    top5_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        if args.distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_loss = train_one_epoch(train_dataloader, ddp_model if args.distributed else model, optimizer, criterion, args, ema_model, scheduler, scaler, epoch)\n",
    "        val_loss, top1, top5 = validate(valid_dataloader, ddp_model if args.distributed else model, valid_criterion, args, 'org')\n",
    "        if args.ema:\n",
    "            eval_ema_metric = validate(valid_dataloader, ema_model.module, valid_criterion, args, 'ema')\n",
    "\n",
    "        if args.use_wandb:\n",
    "            args.log({'train_loss':train_loss, 'val_loss':val_loss, 'top1':top1, 'top5':top5}, metric=True)\n",
    "\n",
    "        if best_acc < top1:\n",
    "            best_acc = top1\n",
    "            best_epoch = epoch\n",
    "        top1_list.append(top1)\n",
    "        top5_list.append(top5)\n",
    "\n",
    "        if args.save_checkpoint and args.is_rank_zero:\n",
    "            save_checkpoint(args.log_dir, model, ema_model, optimizer,\n",
    "                            scaler, scheduler, epoch, is_best=best_epoch == epoch)\n",
    "\n",
    "    # 8. summary train result in csv\n",
    "    if args.is_rank_zero:\n",
    "        best_acc = round(float(best_acc), 4)\n",
    "        top1 = round(float(sum(top1_list[-3:]) / 3), 4)\n",
    "        top5 = round(float(sum(top5_list[-3:]) / 3), 4)\n",
    "        duration = str(datetime.timedelta(seconds=time.time() - start_time)).split('.')[0]\n",
    "        Result(args.output_dir).save_result(args, top1_list, top5_list,\n",
    "                                            dict(duration=duration, best_acc=best_acc, avg_top1_acc=top1, avg_top5_acc=top5))\n",
    "\n",
    "    # 9. save model weight\n",
    "    if args.save_last_epoch and args.is_rank_zero:\n",
    "        save_checkpoint(args.log_dir, model, ema_model, optimizer, scaler, scheduler, end_epoch-1, is_best=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arguments = (\"data --dataset_type CIFAR100 --model-name volo_d1_224 --train-size 224 224 \"\n",
    "    \"--train-resize-mode ResizeRandomCrop --random-crop-pad 28 --test-size 224 224 \"\n",
    "    \"--center-crop-ptr 1.0 --interpolation bicubic --mean 0.4914 0.4825 0.4467 --std 0.2471 0.2435 0.2616 \"\n",
    "    \"--cutmix 1.0 --mixup 0.0 --remode 0.0 --drop-path-rate 0.0 --smoothing 0.0 --epoch 300 --optimizer sgd \"\n",
    "    \"--nesterov --lr 0.25 --min-lr 1e-4 --weight-decay 1e-4 --warmup-epoch 5 --scheduler cosine -b 128 -j 4 \"\n",
    "    \"--pin-memory --amp --channels-last --cuda 8\")\n",
    "    args_parser = get_args_parser()\n",
    "    args = args_parser.parse_args(arguments.split(' '))\n",
    "    run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
